{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bit041d7cde41394c5dbfcab2a1ffa675bc",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz\n100%|█████████▉| 9912320/9912422 [10:01<00:00, 12905.34it/s]Extracting ../../data/MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data/MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n\n0it [00:00, ?it/s]\n  0%|          | 0/28881 [00:03<?, ?it/s]\n 28%|██▊       | 8192/28881 [00:04<00:01, 12250.68it/s]\n 57%|█████▋    | 16384/28881 [00:04<00:01, 12158.44it/s]\n 85%|████████▌ | 24576/28881 [00:05<00:00, 12852.54it/s]Extracting ../../data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n\n\n0it [00:00, ?it/s]\n\n  0%|          | 0/1648877 [00:02<?, ?it/s]\n\n  0%|          | 8192/1648877 [00:02<01:11, 22977.57it/s]\n\n  2%|▏         | 32768/1648877 [00:02<00:55, 29134.96it/s]\n\n  2%|▏         | 40960/1648877 [00:03<00:56, 28708.70it/s]\n\n  3%|▎         | 57344/1648877 [00:05<01:35, 16668.69it/s]\n\n  4%|▍         | 65536/1648877 [00:05<01:22, 19265.18it/s]\n\n  4%|▍         | 73728/1648877 [00:07<02:39, 9879.66it/s]\n\n  5%|▌         | 90112/1648877 [00:07<02:15, 11471.37it/s]\n\n  6%|▌         | 98304/1648877 [00:08<02:24, 10764.15it/s]\n\n9920512it [10:17, 12905.34it/s]\n\n  7%|▋         | 114688/1648877 [00:09<02:03, 12415.52it/s]\n\n  7%|▋         | 122880/1648877 [00:10<01:41, 15022.88it/s]\n\n  8%|▊         | 131072/1648877 [00:10<01:25, 17689.78it/s]\n\n  8%|▊         | 139264/1648877 [00:10<01:14, 20186.05it/s]\n\n  9%|▉         | 147456/1648877 [00:11<01:07, 22104.55it/s]\n\n  9%|▉         | 155648/1648877 [00:11<01:03, 23543.78it/s]\n\n 10%|▉         | 163840/1648877 [00:11<00:59, 24764.20it/s]\n\n 10%|█         | 172032/1648877 [00:11<00:56, 26191.62it/s]\n\n 11%|█         | 180224/1648877 [00:12<00:54, 27178.16it/s]\n\n 12%|█▏        | 196608/1648877 [00:12<00:45, 32112.60it/s]\n\n 12%|█▏        | 204800/1648877 [00:13<01:34, 15251.52it/s]\n\n 13%|█▎        | 221184/1648877 [00:14<01:15, 19000.82it/s]\n\n 14%|█▍        | 229376/1648877 [00:14<01:06, 21310.62it/s]\n\n 14%|█▍        | 237568/1648877 [00:16<02:25, 9731.20it/s]\n\n 15%|█▍        | 245760/1648877 [00:16<01:55, 12154.38it/s]\n\n 15%|█▌        | 253952/1648877 [00:16<01:35, 14598.37it/s]\n\n 16%|█▌        | 262144/1648877 [00:17<01:20, 17162.98it/s]\n\n 16%|█▋        | 270336/1648877 [00:17<01:10, 19649.83it/s]\n\n 17%|█▋        | 278528/1648877 [00:17<01:03, 21482.90it/s]\n\n 18%|█▊        | 294912/1648877 [00:17<00:50, 26557.55it/s]\n\n 18%|█▊        | 303104/1648877 [00:18<00:49, 27175.44it/s]\n\n 19%|█▉        | 311296/1648877 [00:18<00:48, 27838.30it/s]\n\n 20%|█▉        | 327680/1648877 [00:18<00:39, 33048.23it/s]\n32768it [00:24, 12852.54it/s]\n\n 20%|██        | 335872/1648877 [00:20<01:32, 14162.40it/s]\n\n 21%|██        | 344064/1648877 [00:20<01:30, 14362.69it/s]\n\n 22%|██▏       | 360448/1648877 [00:20<01:09, 18600.12it/s]\n\n 22%|██▏       | 368640/1648877 [00:21<01:00, 20999.03it/s]\n\n 23%|██▎       | 385024/1648877 [00:21<00:48, 26059.49it/s]\n\n 24%|██▍       | 393216/1648877 [00:21<00:46, 27075.82it/s]\n\n 25%|██▍       | 409600/1648877 [00:22<00:38, 32396.11it/s]\n\n 25%|██▌       | 417792/1648877 [00:23<01:21, 15141.97it/s]\n\n 27%|██▋       | 442368/1648877 [00:24<01:07, 17914.60it/s]\n\n 27%|██▋       | 450560/1648877 [00:24<00:58, 20341.21it/s]\n\n 28%|██▊       | 466944/1648877 [00:24<00:46, 25378.47it/s]\n\n 29%|██▉       | 475136/1648877 [00:25<00:55, 21163.31it/s]\n\n 30%|██▉       | 491520/1648877 [00:25<00:44, 26277.22it/s]\n\n 31%|███       | 507904/1648877 [00:25<00:36, 31439.17it/s]\n\n 31%|███▏      | 516096/1648877 [00:25<00:36, 30931.74it/s]\n\n 32%|███▏      | 532480/1648877 [00:26<00:30, 36184.14it/s]\n\n 33%|███▎      | 548864/1648877 [00:28<00:58, 18724.11it/s]\n\n 34%|███▍      | 557056/1648877 [00:28<01:05, 16614.69it/s]\n\n 35%|███▍      | 573440/1648877 [00:29<00:56, 19150.80it/s]\n\n 36%|███▌      | 589824/1648877 [00:29<00:50, 20783.28it/s]\n\n 36%|███▋      | 598016/1648877 [00:30<01:05, 16093.24it/s]\n\n 37%|███▋      | 606208/1648877 [00:31<00:58, 17765.67it/s]\n\n 37%|███▋      | 614400/1648877 [00:31<01:01, 16804.73it/s]\n\n 38%|███▊      | 622592/1648877 [00:34<02:15, 7583.75it/s]\n\n 38%|███▊      | 630784/1648877 [00:34<01:44, 9772.79it/s]\n\n 39%|███▉      | 638976/1648877 [00:34<01:32, 10874.32it/s]\n\n 39%|███▉      | 647168/1648877 [00:35<01:14, 13453.86it/s]\n\n 40%|███▉      | 655360/1648877 [00:35<01:01, 16095.15it/s]\n\n 40%|████      | 663552/1648877 [00:35<00:52, 18595.88it/s]\n\n 41%|████      | 671744/1648877 [00:35<00:46, 20970.28it/s]\n\n 41%|████      | 679936/1648877 [00:36<00:42, 23031.17it/s]\n\n 42%|████▏     | 696320/1648877 [00:36<00:33, 28152.98it/s]\n\n 43%|████▎     | 704512/1648877 [00:37<01:02, 15216.45it/s]\n\n 44%|████▍     | 729088/1648877 [00:37<00:45, 20254.86it/s]\n\n 45%|████▍     | 737280/1648877 [00:38<00:40, 22430.14it/s]\n\n 46%|████▌     | 753664/1648877 [00:40<01:03, 14137.52it/s]\n\n 47%|████▋     | 770048/1648877 [00:40<00:52, 16794.25it/s]\n\n 47%|████▋     | 778240/1648877 [00:42<01:15, 11543.77it/s]\n\n 49%|████▊     | 802816/1648877 [00:42<00:58, 14361.51it/s]\n\n 49%|████▉     | 811008/1648877 [00:43<00:51, 16221.03it/s]\n\n 50%|████▉     | 819200/1648877 [00:43<00:44, 18839.25it/s]\n\n 50%|█████     | 827392/1648877 [00:44<00:49, 16582.76it/s]\n\n 51%|█████     | 835584/1648877 [00:45<01:09, 11709.77it/s]\n\n 51%|█████     | 843776/1648877 [00:45<01:04, 12525.29it/s]\n\n 52%|█████▏    | 851968/1648877 [00:46<01:00, 13126.16it/s]\n\n 52%|█████▏    | 860160/1648877 [00:46<00:49, 15783.04it/s]\n\n 53%|█████▎    | 868352/1648877 [00:48<01:20, 9743.60it/s]\n\n 53%|█████▎    | 876544/1648877 [00:48<01:03, 12221.99it/s]\n\n 54%|█████▎    | 884736/1648877 [00:49<00:59, 12868.89it/s]\n\n 54%|█████▍    | 892928/1648877 [00:49<00:48, 15531.66it/s]\n\n 55%|█████▍    | 901120/1648877 [00:50<01:05, 11351.57it/s]\n\n 55%|█████▌    | 909312/1648877 [00:50<00:51, 14374.82it/s]\n\n 56%|█████▌    | 917504/1648877 [00:51<00:53, 13748.67it/s]\n\n 56%|█████▌    | 925696/1648877 [00:51<00:51, 14078.92it/s]\n\n 57%|█████▋    | 933888/1648877 [00:52<00:42, 16719.25it/s]\n\n 57%|█████▋    | 942080/1648877 [00:52<00:36, 19260.37it/s]\n\n 58%|█████▊    | 950272/1648877 [00:52<00:32, 21619.06it/s]\n\n 58%|█████▊    | 958464/1648877 [00:53<00:29, 23585.84it/s]\n\n 59%|█████▊    | 966656/1648877 [00:53<00:41, 16515.71it/s]\n\n 59%|█████▉    | 974848/1648877 [00:54<00:42, 15912.78it/s]\n\n 60%|██████    | 991232/1648877 [00:56<00:54, 11983.02it/s]\n\n 61%|██████    | 999424/1648877 [00:56<00:44, 14636.53it/s]\n\n 61%|██████    | 1007616/1648877 [00:57<00:49, 12957.84it/s]\n\n 62%|██████▏   | 1015808/1648877 [00:58<00:47, 13262.60it/s]\n\n 62%|██████▏   | 1024000/1648877 [00:58<00:42, 14819.17it/s]\n\n 63%|██████▎   | 1032192/1648877 [00:59<00:38, 16045.65it/s]\n\n 63%|██████▎   | 1040384/1648877 [00:59<00:35, 17034.30it/s]\n\n 64%|██████▎   | 1048576/1648877 [01:00<00:53, 11214.52it/s]\n\n 64%|██████▍   | 1056768/1648877 [01:01<00:54, 10855.00it/s]\n\n 65%|██████▍   | 1064960/1648877 [01:02<00:55, 10587.36it/s]\n\n 65%|██████▌   | 1073152/1648877 [01:02<00:49, 11552.05it/s]\n\n 66%|██████▌   | 1081344/1648877 [01:04<00:55, 10193.71it/s]\n\n 66%|██████▌   | 1089536/1648877 [01:05<01:11, 7776.34it/s]\n\n 67%|██████▋   | 1097728/1648877 [01:07<01:18, 7049.84it/s]\n\n 67%|██████▋   | 1105920/1648877 [01:08<01:25, 6363.61it/s]\n\n 68%|██████▊   | 1114112/1648877 [01:09<01:16, 6998.87it/s]\n\n 68%|██████▊   | 1122304/1648877 [01:10<01:03, 8324.52it/s]\n\n 69%|██████▊   | 1130496/1648877 [01:10<00:53, 9620.90it/s]\n\n 69%|██████▉   | 1138688/1648877 [01:11<00:56, 8994.87it/s]\n\n 70%|██████▉   | 1146880/1648877 [01:13<01:08, 7334.57it/s]\n\n 70%|███████   | 1155072/1648877 [01:13<00:57, 8657.79it/s]\n\n 71%|███████   | 1163264/1648877 [01:14<00:49, 9888.21it/s]\n\n 71%|███████   | 1171456/1648877 [01:14<00:38, 12382.78it/s]\n\n 72%|███████▏  | 1179648/1648877 [01:16<00:54, 8675.78it/s]\n\n 72%|███████▏  | 1187840/1648877 [01:16<00:41, 11039.35it/s]\n\n 73%|███████▎  | 1196032/1648877 [01:17<00:37, 11969.35it/s]\n\n 73%|███████▎  | 1204224/1648877 [01:17<00:34, 12722.54it/s]\n\n 74%|███████▍  | 1220608/1648877 [01:18<00:29, 14400.72it/s]\n\n 75%|███████▍  | 1228800/1648877 [01:18<00:24, 17099.09it/s]\n\n 75%|███████▌  | 1236992/1648877 [01:19<00:22, 18682.28it/s]\n\n 76%|███████▌  | 1245184/1648877 [01:19<00:23, 17422.87it/s]\n\n 76%|███████▌  | 1253376/1648877 [01:19<00:19, 19888.76it/s]\n\n 77%|███████▋  | 1261568/1648877 [01:20<00:17, 22159.51it/s]\n\n 77%|███████▋  | 1269760/1648877 [01:20<00:20, 18397.16it/s]\n\n 78%|███████▊  | 1277952/1648877 [01:21<00:20, 18121.93it/s]\n\n 78%|███████▊  | 1286144/1648877 [01:21<00:18, 19481.34it/s]\n\n 78%|███████▊  | 1294336/1648877 [01:21<00:16, 21758.37it/s]\n\n 79%|███████▉  | 1302528/1648877 [01:23<00:27, 12439.53it/s]\n\n 80%|███████▉  | 1318912/1648877 [01:27<00:43, 7633.23it/s]\n\n 80%|████████  | 1327104/1648877 [01:27<00:33, 9688.29it/s]\n\n 81%|████████  | 1335296/1648877 [01:28<00:28, 10846.23it/s]\n\n 81%|████████▏ | 1343488/1648877 [01:28<00:28, 10538.67it/s]\n\n 82%|████████▏ | 1359872/1648877 [01:29<00:22, 12964.57it/s]\n\n 83%|████████▎ | 1368064/1648877 [01:29<00:18, 15043.46it/s]\n\n 83%|████████▎ | 1376256/1648877 [01:30<00:15, 17687.43it/s]\n\n 84%|████████▍ | 1384448/1648877 [01:30<00:15, 16722.48it/s]\n\n 84%|████████▍ | 1392640/1648877 [01:31<00:15, 16208.37it/s]\n\n 85%|████████▍ | 1400832/1648877 [01:31<00:13, 18814.64it/s]\n\n 85%|████████▌ | 1409024/1648877 [01:31<00:11, 20020.09it/s]\n\n 86%|████████▌ | 1417216/1648877 [01:32<00:10, 22278.23it/s]\n\n 86%|████████▋ | 1425408/1648877 [01:32<00:09, 23337.02it/s]\n\n 87%|████████▋ | 1433600/1648877 [01:32<00:08, 24872.06it/s]\n\n 87%|████████▋ | 1441792/1648877 [01:32<00:07, 26070.75it/s]\n\n 88%|████████▊ | 1449984/1648877 [01:33<00:07, 26736.85it/s]\n\n 88%|████████▊ | 1458176/1648877 [01:33<00:08, 21619.42it/s]\n\n 89%|████████▉ | 1474560/1648877 [01:34<00:09, 19220.27it/s]\n\n 90%|████████▉ | 1482752/1648877 [01:35<00:11, 14758.35it/s]\n\n 90%|█████████ | 1490944/1648877 [01:36<00:10, 14865.23it/s]\n\n 91%|█████████ | 1499136/1648877 [01:36<00:08, 17527.80it/s]\n\n 91%|█████████▏| 1507328/1648877 [01:36<00:07, 20025.23it/s]\n\n 92%|█████████▏| 1515520/1648877 [01:38<00:12, 10702.33it/s]\n\n 92%|█████████▏| 1523712/1648877 [01:39<00:14, 8576.16it/s]\n\n 93%|█████████▎| 1531904/1648877 [01:40<00:13, 8977.08it/s]\n\n 93%|█████████▎| 1540096/1648877 [01:40<00:09, 11366.24it/s]\n\n 94%|█████████▍| 1548288/1648877 [01:41<00:08, 12279.86it/s]\n\n 94%|█████████▍| 1556480/1648877 [01:41<00:06, 14920.72it/s]\n\n 95%|█████████▍| 1564672/1648877 [01:41<00:04, 17547.65it/s]\n\n 95%|█████████▌| 1572864/1648877 [01:42<00:03, 20015.11it/s]\n\n 96%|█████████▌| 1581056/1648877 [01:42<00:03, 22208.56it/s]\n\n 96%|█████████▋| 1589248/1648877 [01:42<00:02, 24044.10it/s]\n\n 97%|█████████▋| 1605632/1648877 [01:44<00:02, 17224.81it/s]\n\n 98%|█████████▊| 1613824/1648877 [01:44<00:01, 19751.89it/s]\n\n 98%|█████████▊| 1622016/1648877 [01:44<00:01, 21874.90it/s]\n\n 99%|█████████▉| 1630208/1648877 [01:45<00:00, 23719.54it/s]\n\n 99%|█████████▉| 1638400/1648877 [01:45<00:00, 25377.55it/s]\n\n100%|█████████▉| 1646592/1648877 [01:45<00:00, 26520.86it/s]Extracting ../../data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data/MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n\n\n\n0it [00:00, ?it/s]\n\n\n8192it [00:01, 5098.08it/s]\nExtracting ../../data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data/MNIST\\raw\nProcessing...\nDone!\n"
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "#Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dense1 = nn.Linear(12*12*64, 128)\n",
    "        self.dense2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.view(-1, 12*12*64)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.dense2(x)\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CNN(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dense1): Linear(in_features=9216, out_features=128, bias=True)\n  (dense2): Linear(in_features=128, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "total_step = len(train_loader)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch[1/15], Step[100/469], Loss:0.4211\nEpoch[1/15], Step[200/469], Loss:0.1153\nEpoch[1/15], Step[300/469], Loss:0.1231\nEpoch[1/15], Step[400/469], Loss:0.1589\nEpoch[2/15], Step[100/469], Loss:0.1272\nEpoch[2/15], Step[200/469], Loss:0.1699\nEpoch[2/15], Step[300/469], Loss:0.2045\nEpoch[2/15], Step[400/469], Loss:0.0655\nEpoch[3/15], Step[100/469], Loss:0.0392\nEpoch[3/15], Step[200/469], Loss:0.0685\nEpoch[3/15], Step[300/469], Loss:0.0422\nEpoch[3/15], Step[400/469], Loss:0.0998\nEpoch[4/15], Step[100/469], Loss:0.0410\nEpoch[4/15], Step[200/469], Loss:0.0979\nEpoch[4/15], Step[300/469], Loss:0.0166\nEpoch[4/15], Step[400/469], Loss:0.0319\nEpoch[5/15], Step[100/469], Loss:0.0387\nEpoch[5/15], Step[200/469], Loss:0.0499\nEpoch[5/15], Step[300/469], Loss:0.0479\nEpoch[5/15], Step[400/469], Loss:0.0348\nEpoch[6/15], Step[100/469], Loss:0.0746\nEpoch[6/15], Step[200/469], Loss:0.0455\nEpoch[6/15], Step[300/469], Loss:0.1199\nEpoch[6/15], Step[400/469], Loss:0.0685\nEpoch[7/15], Step[100/469], Loss:0.0396\nEpoch[7/15], Step[200/469], Loss:0.0326\nEpoch[7/15], Step[300/469], Loss:0.0632\nEpoch[7/15], Step[400/469], Loss:0.0594\nEpoch[8/15], Step[100/469], Loss:0.0388\nEpoch[8/15], Step[200/469], Loss:0.0122\nEpoch[8/15], Step[300/469], Loss:0.0323\nEpoch[8/15], Step[400/469], Loss:0.0365\nEpoch[9/15], Step[100/469], Loss:0.0867\nEpoch[9/15], Step[200/469], Loss:0.0254\nEpoch[9/15], Step[300/469], Loss:0.0067\nEpoch[9/15], Step[400/469], Loss:0.0484\nEpoch[10/15], Step[100/469], Loss:0.1286\nEpoch[10/15], Step[200/469], Loss:0.0161\nEpoch[10/15], Step[300/469], Loss:0.0578\nEpoch[10/15], Step[400/469], Loss:0.0947\nEpoch[11/15], Step[100/469], Loss:0.0079\nEpoch[11/15], Step[200/469], Loss:0.0132\nEpoch[11/15], Step[300/469], Loss:0.0153\nEpoch[11/15], Step[400/469], Loss:0.0381\nEpoch[12/15], Step[100/469], Loss:0.0217\nEpoch[12/15], Step[200/469], Loss:0.0314\nEpoch[12/15], Step[300/469], Loss:0.0118\nEpoch[12/15], Step[400/469], Loss:0.0161\nEpoch[13/15], Step[100/469], Loss:0.0267\nEpoch[13/15], Step[200/469], Loss:0.0054\nEpoch[13/15], Step[300/469], Loss:0.0548\nEpoch[13/15], Step[400/469], Loss:0.0096\nEpoch[14/15], Step[100/469], Loss:0.0465\nEpoch[14/15], Step[200/469], Loss:0.0043\nEpoch[14/15], Step[300/469], Loss:0.0208\nEpoch[14/15], Step[400/469], Loss:0.0066\nEpoch[15/15], Step[100/469], Loss:0.0060\nEpoch[15/15], Step[200/469], Loss:0.0018\nEpoch[15/15], Step[300/469], Loss:0.0329\nEpoch[15/15], Step[400/469], Loss:0.0303\n"
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch[{}/{}], Step[{}/{}], Loss:{:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Test Accuracy of the model on the 10000 test images: 98.37%\nAUC: 99.09%\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "preds = []\n",
    "y_true = []\n",
    "# Test the model\n",
    "model.eval()  # Set model to evaluation mode.\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()    \n",
    "        detached_pred = predicted.detach().cpu().numpy()\n",
    "        detached_label = labels.detach().cpu().numpy()\n",
    "        for f in range(0, len(detached_pred)):\n",
    "            preds.append(detached_pred[f])\n",
    "            y_true.append(detached_label[f])\n",
    "        \n",
    "    print('Test Accuracy of the model on the 10000 test images: {:.2%}'.format(correct / total))\n",
    "    \n",
    "    preds = np.eye(num_classes)[preds]\n",
    "    y_true = np.eye(num_classes)[y_true]    \n",
    "    auc = roc_auc_score(preds, y_true)\n",
    "    print(\"AUC: {:.2%}\".format (auc))\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'pytorch_mnist_cnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]\n ...\n [ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]\n [ True  True  True ...  True  True  True]]\nAUC: 99.14%\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predss = []\n",
    "    matches = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for f in range(0, len(predicted.detach().cpu().numpy())):\n",
    "            predss.append(predicted.detach().cpu().numpy()[f])\n",
    "            matches.append(labels.detach().cpu().numpy()[f])\n",
    "            \n",
    "#     preds = np.eye(num_classes)[predicted.detach().cpu().numpy()]\n",
    "#     y_true = np.eye(num_classes)[labels.detach().cpu().numpy()]\n",
    "#     print(predss, matches)\n",
    "    preds = np.eye(num_classes)[predss]\n",
    "    y_true = np.eye(num_classes)[matches]\n",
    "    \n",
    "    print(preds == y_true)\n",
    "    auc = roc_auc_score(preds, y_true)\n",
    "    print(\"AUC: {:.2%}\".format (auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ConvNet(\n  (layer1): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n  )\n  (layer2): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer3): Sequential()\n  (fc): Linear(in_features=1568, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            \n",
    "        \n",
    "        )\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn.Sequential(Recommended)\n",
    "一般情况下 nn.Sequential 的用法是来组成卷积块 (block)，然后像拼积木一样把不同的 block 拼成整个网络，让代码更简洁，更加结构化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Net2(\n  (conv): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (dense): Sequential(\n    (0): Linear(in_features=288, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv = nn.Sequential(         # A sequential container\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),     # nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(32 * 3 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv1(x)\n",
    "        res = conv_out.view(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    "\n",
    "model2 = Net2()\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn.ModuleList\n",
    "是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。若使用python的list，则会出问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "net_modlist(\n  (modlist): ModuleList(\n    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n    (3): ReLU()\n  )\n)\n<class 'torch.Tensor'>torch.Size([20, 1, 5, 5])\n<class 'torch.Tensor'>torch.Size([20])\n<class 'torch.Tensor'>torch.Size([64, 20, 5, 5])\n<class 'torch.Tensor'>torch.Size([64])\n"
    }
   ],
   "source": [
    "class net_modlist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_modlist, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net_modlist = net_modlist()\n",
    "print(net_modlist)\n",
    "for param in net_modlist.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "net_modlist()\n"
    }
   ],
   "source": [
    "class net_modlist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_modlist, self).__init__()\n",
    "        self.modlist = [\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "net_modlist = net_modlist()\n",
    "print(net_modlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn.Sequential与nn.ModuleList的区别\n",
    "### 1. nn.Sequential内部实现了forward函数，因此可以不用写forward函数。而nn.ModuleList则没有\n",
    "### 2. nn.Sequential可以使用OrderedDict对每层进行命名\n",
    "### 3. nn.Sequential里面的模块按照顺序进行排列的，所以必须确保前一个模块的输出大小和下一个模块的输入大小是一致的。而nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序，网络的执行顺序是根据 forward 函数来决定的\n",
    "### 4. nn.ModuleList有的时候网络中有很多相似或者重复的层，我们一般会考虑用for循环来创建它们，而不是一行一行地写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sequential(\n  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n  (3): ReLU()\n)\n"
    }
   ],
   "source": [
    "seq = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([16, 64, 12, 12])\n"
    }
   ],
   "source": [
    "input = torch.randn(16, 1, 20, 20)\n",
    "print(seq(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([16, 64, 12, 12])\n"
    }
   ],
   "source": [
    "# 继承nn.Module类的话，就要写出forward函数\n",
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "                        nn.Conv2d(1,20,5),\n",
    "                         nn.ReLU(),\n",
    "                          nn.Conv2d(20,64,5),\n",
    "                       nn.ReLU()\n",
    "                       )      \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "    #注意：按照下面这种利用for循环的方式也是可以得到同样结果的\n",
    "    #def forward(self, x):\n",
    "    #    for s in self.seq:\n",
    "    #        x = s(x)\n",
    "    #    return x\n",
    "\n",
    " #对net1进行输入\n",
    "input = torch.randn(16, 1, 20, 20)\n",
    "net1 = net1()\n",
    "print(net1(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ModuleList(\n  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n  (3): ReLU()\n)\n"
    }
   ],
   "source": [
    "# 不写forward报错\n",
    "modlist = nn.ModuleList([\n",
    "         nn.Conv2d(1, 20, 5),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(20, 64, 5),\n",
    "         nn.ReLU()\n",
    "         ])\n",
    "print(modlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7d0d1790db65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input = torch.randn(16, 1, 20, 20)\n",
    "print(modlist(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([16, 64, 12, 12])\n"
    }
   ],
   "source": [
    "# 写出forward函数\n",
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "        self.modlist = nn.ModuleList([\n",
    "                       nn.Conv2d(1, 20, 5),\n",
    "                       nn.ReLU(),\n",
    "                        nn.Conv2d(20, 64, 5),\n",
    "                        nn.ReLU()\n",
    "                        ])\n",
    "\n",
    "    #这里若按照这种写法则会报NotImplementedError错\n",
    "    #def forward(self, x):\n",
    "    #    return self.modlist(x)\n",
    "\n",
    "    #注意：只能按照下面利用for循环的方式\n",
    "    def forward(self, x):\n",
    "        for m in self.modlist:\n",
    "            x = m(x)\n",
    "        return x\n",
    "\n",
    "input = torch.randn(16, 1, 20, 20)\n",
    "net2 = net2()\n",
    "print(net2(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sequential(\n  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n  (ReLU1): ReLU()\n  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n  (ReLU2): ReLU()\n)\n"
    }
   ],
   "source": [
    "# 使用OrderedDict对每层进行命名\n",
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('ReLU1', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('ReLU2', nn.ReLU())\n",
    "    ]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "net3(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=20, bias=True)\n    (1): Linear(in_features=20, out_features=30, bias=True)\n    (2): Linear(in_features=5, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# nn.module不要求顺序\n",
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[2](x)\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net3 = net3()\n",
    "print(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "net4(\n  (linears): ModuleList(\n    (0): Linear(in_features=10, out_features=10, bias=True)\n    (1): Linear(in_features=10, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n    (3): Linear(in_features=10, out_features=10, bias=True)\n    (4): Linear(in_features=10, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "# nn.ModuleList批量生成网络\n",
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4, self).__init__()\n",
    "        layers = [nn.Linear(10, 10) for i in range(5)]\n",
    "        self.linears = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "net = net4()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}